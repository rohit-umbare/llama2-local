# llama2-local
running llama locally using nvidia api
